# Pretraining Configuration
# Static values that don't depend on runtime device/memory detection

# I/O settings
out_dir: "out"
eval_interval: 100
log_interval: 200
eval_iters: 200
eval_only: false
always_save_checkpoint: true
init_from: "scratch"

# Logging
wandb_log: true

# Model architecture
n_layer: 8
n_head: 16
n_embd: 512
dropout: 0.0
bias: true

# Optimizer settings
learning_rate: 0.0003
n_epochs: 10
weight_decay: 0.01
beta1: 0.9
beta2: 0.95
grad_clip: 1.0

# Learning rate schedule
decay_lr: true
use_pytorch_scheduler: true  # Use PyTorch's built-in scheduler for better stability
warmup_iters: 500
lr_decay_iters: 5000
min_lr: 0.00003  # 10% of peak learning rate

# Process settings (static defaults)
master_process: true
seed_offset: 0
ddp_world_size: 1
